<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Tutorial • abcgof</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Tutorial">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">abcgof</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="active nav-item"><a class="nav-link" href="../articles/gaussian_laplace.html">Tutorial</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Tutorial</h1>
            
      

      <div class="d-none name"><code>gaussian_laplace.Rmd</code></div>
    </div>

    
    
<div class="section level2">
<h2 id="toy-laplace---gaussian-example">Toy Laplace - Gaussian Example<a class="anchor" aria-label="anchor" href="#toy-laplace---gaussian-example"></a>
</h2>
<p>In this tutorial, we use a toy “Laplace - Gaussian” dataset
(<code>gaussian_laplace</code>), that is embedded in the
<code>abcgof</code> package.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://pbastide.github.io/abcgof/">abcgof</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"gaussian_laplace"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/attach.html" class="external-link">attach</a></span><span class="op">(</span><span class="va">gaussian_laplace</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html" class="external-link">set.seed</a></span><span class="op">(</span><span class="fl">1289</span><span class="op">)</span></span></code></pre></div>
<p>We assume that the reference table is drawn using the following model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>,
that uses a Laplace distribution:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msub><mi>m</mi><mn>0</mn></msub><mrow><mtext mathvariant="normal">: </mtext><mspace width="0.333em"></mspace></mrow></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>θ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>μ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mtext mathvariant="normal">Uniform</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>σ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mtext mathvariant="normal">Uniform</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>z</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mover><mo>∼</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><mtext mathvariant="normal">Laplace</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>σ</mi><mi>/</mi><msqrt><mn>2</mn></msqrt><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for </mtext><mspace width="0.333em"></mspace></mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>d</mi><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">L-moments</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo>;</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
   \text{$m_0$: }
      \left\{
   \begin{aligned}
   \theta &amp;= (\mu, \sigma); \\
   \mu &amp;\sim \mbox{Uniform}(-5,5); \\
   \sigma &amp;\sim \mbox{Uniform}(1,4);\\
    z_i &amp;\overset{iid}{\sim} \mbox{Laplace}(\mu,\sigma/\sqrt{2}) \mbox{ for } 1 \leq i \leq d;\\
    y &amp;= \text{L-moments}(z; m);
    \end{aligned}
    \right.
\end{equation*}</annotation></semantics></math> where the simulated
vectors
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
has dimension
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>350</mn></mrow><annotation encoding="application/x-tex">d=350</annotation></semantics></math>
and is not observed directly, but instead summarized using the first
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">m = 20</annotation></semantics></math>
sample L-moments ratios using function <code>salmu</code> from package
<code>lmom</code>.</p>
<p>The simulated parameters and reference table are stored in the
<code>gaussian_laplace</code> object, along with the simulation
function. The reference table has 1000 rows.</p>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">sim_params</span> <span class="op">&lt;-</span> <span class="va">dataset_laplace</span><span class="op">$</span><span class="va">param</span></span>
<span><span class="va">sumstats</span> <span class="op">&lt;-</span> <span class="va">dataset_laplace</span><span class="op">$</span><span class="va">sim</span></span>
<span><span class="va">sim_function</span> <span class="op">&lt;-</span> <span class="va">dataset_laplace</span><span class="op">$</span><span class="va">sim.fun</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sim_params</span><span class="op">)</span></span>
<span><span class="co">#&gt;              mu     scale</span></span>
<span><span class="co">#&gt; [1,] -4.9483335 1.3464334</span></span>
<span><span class="co">#&gt; [2,]  1.2191817 1.9634809</span></span>
<span><span class="co">#&gt; [3,] -0.8857967 1.8678978</span></span>
<span><span class="co">#&gt; [4,]  0.7643363 1.9228813</span></span>
<span><span class="co">#&gt; [5,] -6.1565456 0.7325722</span></span>
<span><span class="co">#&gt; [6,]  2.9305190 1.7523864</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">sumstats</span><span class="op">)</span></span>
<span><span class="co">#&gt;             l_1       l_2          t_3       t_4          t_5        t_6</span></span>
<span><span class="co">#&gt; [1,] -4.9344853 1.0271073  0.048835212 0.2226024  0.023550314 0.06886227</span></span>
<span><span class="co">#&gt; [2,]  1.1850598 1.4876990 -0.025005303 0.2211680  0.006650818 0.09179471</span></span>
<span><span class="co">#&gt; [3,] -0.7398060 1.4306265  0.008943995 0.2518546  0.001105052 0.11031211</span></span>
<span><span class="co">#&gt; [4,]  0.6918504 1.3695172  0.021150078 0.2022437 -0.011552312 0.05681890</span></span>
<span><span class="co">#&gt; [5,] -6.1508125 0.5601383  0.078462189 0.2592043  0.053934372 0.08787851</span></span>
<span><span class="co">#&gt; [6,]  2.9109914 1.4306385  0.027822062 0.2590423  0.043137700 0.11603878</span></span>
<span><span class="co">#&gt;               t_7        t_8           t_9       t_10         t_11       t_12</span></span>
<span><span class="co">#&gt; [1,]  0.020564264 0.04687520  0.0024936118 0.01915359 -0.009335724 0.00165149</span></span>
<span><span class="co">#&gt; [2,]  0.002258141 0.05415833 -0.0036408789 0.03663786 -0.004957587 0.02893470</span></span>
<span><span class="co">#&gt; [3,]  0.010106990 0.04970803  0.0119567379 0.02511239  0.008432557 0.01929786</span></span>
<span><span class="co">#&gt; [4,] -0.013831907 0.02429899  0.0009273301 0.02336813  0.005315742 0.02175305</span></span>
<span><span class="co">#&gt; [5,]  0.018261429 0.05200649 -0.0024320013 0.03566424 -0.008800341 0.03219378</span></span>
<span><span class="co">#&gt; [6,]  0.027837707 0.07015139  0.0123667142 0.04342968  0.012087101 0.03575983</span></span>
<span><span class="co">#&gt;               t_13         t_14          t_15         t_16         t_17</span></span>
<span><span class="co">#&gt; [1,] -2.148276e-03 -0.004010197 -0.0023968345 -0.004706272 -0.003220299</span></span>
<span><span class="co">#&gt; [2,] -2.901356e-03  0.019641051 -0.0007584820  0.014332985 -0.003596413</span></span>
<span><span class="co">#&gt; [3,]  3.379683e-06  0.015284317  0.0041880260  0.015000429  0.002820463</span></span>
<span><span class="co">#&gt; [4,] -2.891049e-03  0.009568189 -0.0049958836  0.011079600 -0.002835480</span></span>
<span><span class="co">#&gt; [5,] -1.020638e-02  0.026980234 -0.0087784110  0.020902127 -0.011993138</span></span>
<span><span class="co">#&gt; [6,]  7.539844e-03  0.027024252 -0.0002633797  0.016382516 -0.002023180</span></span>
<span><span class="co">#&gt;             t_18         t_19        t_20</span></span>
<span><span class="co">#&gt; [1,] 0.001801288  0.001034639 0.003131637</span></span>
<span><span class="co">#&gt; [2,] 0.007500890 -0.005243057 0.007801159</span></span>
<span><span class="co">#&gt; [3,] 0.010241943  0.007045583 0.011755243</span></span>
<span><span class="co">#&gt; [4,] 0.009010877 -0.009696633 0.005414873</span></span>
<span><span class="co">#&gt; [5,] 0.015172543 -0.014516576 0.013414395</span></span>
<span><span class="co">#&gt; [6,] 0.014394676 -0.003442754 0.013979047</span></span>
<span><span class="va">sim_function</span></span>
<span><span class="co">#&gt; function(params, ...) {</span></span>
<span><span class="co">#&gt;     t(apply(params, 1, function(oneparam) lmom::samlmu(VGAM::rlaplace(350, location = oneparam[1], scale = oneparam[2]), nmom = 20)))</span></span>
<span><span class="co">#&gt;   }</span></span>
<span><span class="co">#&gt; &lt;bytecode: 0x555e1462e1a0&gt;</span></span>
<span><span class="co">#&gt; &lt;environment: 0x555e14627550&gt;</span></span></code></pre></div>
<p>We then assume that we have several observed vector of statistics,
that come from an alternative model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>1</mn></msub><annotation encoding="application/x-tex">m_1</annotation></semantics></math>
that uses a Gaussian model instead:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><msub><mi>m</mi><mn>1</mn></msub><mrow><mtext mathvariant="normal">: </mtext><mspace width="0.333em"></mspace></mrow></mrow><mrow><mo stretchy="true" form="prefix">{</mo><mtable><mtr><mtd columnalign="right" style="text-align: right"><mi>θ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><mi>σ</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>μ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mtext mathvariant="normal">Uniform</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mo>−</mo><mn>5</mn><mo>,</mo><mn>5</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>σ</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>∼</mo><mtext mathvariant="normal">Uniform</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>,</mo><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><msub><mi>z</mi><mi>i</mi></msub></mtd><mtd columnalign="left" style="text-align: left"><mover><mo>∼</mo><mrow><mi>i</mi><mi>i</mi><mi>d</mi></mrow></mover><mtext mathvariant="normal">Normal</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>μ</mi><mo>,</mo><msup><mi>σ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mspace width="0.333em"></mspace><mtext mathvariant="normal"> for </mtext><mspace width="0.333em"></mspace></mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>d</mi><mo>;</mo></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"><mi>y</mi></mtd><mtd columnalign="left" style="text-align: left"><mo>=</mo><mtext mathvariant="normal">L-moments</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>z</mi><mo>;</mo><mi>m</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">\begin{equation*}
   \text{$m_1$: }
   \left\{
   \begin{aligned}
   \theta &amp;= (\mu, \sigma); \\
    \mu &amp;\sim \mbox{Uniform}(-5,5); \\
   \sigma &amp;\sim \mbox{Uniform}(1,4);\\
    z_i &amp;\overset{iid}{\sim} \mbox{Normal}(\mu,\sigma^2) \mbox{ for } 1 \leq i \leq d;\\
    y &amp;= \text{L-moments}(z; m),
    \end{aligned}
    \right.
\end{equation*}</annotation></semantics></math> with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi><mo>=</mo><mn>350</mn></mrow><annotation encoding="application/x-tex">d=350</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">m=20</annotation></semantics></math>
as above.</p>
<p>We take as observed values the first 10 line of the
<code>dataset_gaussian</code> dataset.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_obs</span> <span class="op">&lt;-</span> <span class="va">dataset_gaussian</span><span class="op">$</span><span class="va">sim</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="op">]</span></span></code></pre></div>
<p>Note that both models have conditional mean
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>μ</mi><annotation encoding="application/x-tex">\mu</annotation></semantics></math>
and standard deviations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>σ</mi><annotation encoding="application/x-tex">\sigma</annotation></semantics></math>
for each raw data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>i</mi></msub><annotation encoding="application/x-tex">z_i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>≤</mo><mi>i</mi><mo>≤</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">1 \leq i \leq d</annotation></semantics></math>.
We can use a PCA to visualize the reference statistics and the observed
datasets.</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">trainall</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/cbind.html" class="external-link">rbind</a></span><span class="op">(</span><span class="va">sumstats</span>, <span class="va">y_obs</span><span class="op">)</span></span>
<span><span class="va">ind</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">1</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">sumstats</span><span class="op">)</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">y_obs</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">res.pca</span> <span class="op">&lt;-</span> <span class="fu">stats</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/r/stats/prcomp.html" class="external-link">prcomp</a></span><span class="op">(</span><span class="va">trainall</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">res.pca</span><span class="op">$</span><span class="va">x</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>,<span class="fl">2</span><span class="op">)</span><span class="op">]</span>, col <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"black"</span>, <span class="st">"#D55E00"</span><span class="op">)</span><span class="op">[</span><span class="va">ind</span><span class="op">]</span>, pch <span class="op">=</span> <span class="st">"*"</span><span class="op">)</span></span></code></pre></div>
<p><img src="gaussian_laplace_files/figure-html/plot-1.png" width="576"></p>
<p>Points from the reference table are in black, while observed points
are in orange. As the Laplace model has heavier tails than the Gaussian
model, rejecting the Laplace model with a Gaussian observation can be
difficult, which is why we selected this configuration. Visual
inspection of the first two PC axis does not reveal that observed orange
points would be “outliers” compared to simulated black points.</p>
</div>
<div class="section level2">
<h2 id="pre-inference-prior-gof-test">Pre-Inference Prior GoF Test<a class="anchor" aria-label="anchor" href="#pre-inference-prior-gof-test"></a>
</h2>
<p>We can first perform a Prior Goodness of fit test, based on whole
generated dataset in <code>sumstat</code>. This test checks the null
hypothesis that the observation is drawn from the prior predictive
distribution of model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>
used to generate the reference table. Rejecting the test means that we
reject the null hypothesis that the data was drawn from the prior
predictive
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>,
i.e. that model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>
fails to correctly account for the observation.</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">resgfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gfit.html">gfit</a></span><span class="op">(</span><span class="va">y_obs</span>, <span class="va">sumstats</span><span class="op">)</span></span>
<span><span class="va">resgfit</span></span>
<span><span class="co">#&gt; Prior GoF analysis</span></span>
<span><span class="co">#&gt; Using the lof with k = max</span></span>
<span><span class="co">#&gt; Number of lines in the reference table: 900 ;</span></span>
<span><span class="co">#&gt; Number of calibration point: 100 ;</span></span>
<span><span class="co">#&gt; Range of k values in the original object: k in [2, 20] ;</span></span>
<span><span class="co">#&gt; Number of target observations: 10 ;</span></span>
<span><span class="co">#&gt; Table of estimate, lower and upper 95% confidance interval pvalues:</span></span>
<span><span class="co">#&gt;       estim       lwr       upr</span></span>
<span><span class="co">#&gt;  [1,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [2,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [3,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [4,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [5,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [6,]  0.07 0.0199921 0.1200079</span></span>
<span><span class="co">#&gt;  [7,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [8,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt;  [9,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt; [10,]  0.00 0.0000000 0.0000000</span></span>
<span><span class="co">#&gt; Confidence intervals are based on asymptotic standard error estimation.</span></span></code></pre></div>
<p>By default, the function sets aside
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>100</mn><annotation encoding="application/x-tex">100</annotation></semantics></math>
points from the reference table for calibration, and use the remaining
points to compute the outlier score. The default outlier score is the
“max-LOF” score for k varying between 2 and 20.</p>
<p>Here, we can see that we clearly reject all observation, except for
number 6, that has an upper confidence interval value above the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0.05</mn><annotation encoding="application/x-tex">0.05</annotation></semantics></math>
threshold. As observations are indeed from a different model than the
null model, so this result is expected.</p>
<p>By default, confidence intervals are based on an asymptotic
criterion, that is fast to compute, but can be inaccurate. To get a
better idea of the uncertainty of the estimation, we can use bootstrap
replicates: instead of drawing calibration points only once from the
reference table, we repeat this operation a given number of times, and
look at the distribution of p-values.</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">resgfitboot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/gfit.html">gfit</a></span><span class="op">(</span><span class="va">y_obs</span><span class="op">[</span><span class="fl">6</span>, <span class="op">]</span>, <span class="va">sumstats</span>,</span>
<span>                    nboot <span class="op">=</span> <span class="fl">10</span>,          <span class="co">## For speed, only 10 replicates, but this should be increased.</span></span>
<span>                    ncores <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>          <span class="co">## The number of cores for parallel computation can be increased.</span></span>
<span><span class="va">resgfitboot</span></span>
<span><span class="co">#&gt; Prior GoF analysis</span></span>
<span><span class="co">#&gt; Using the lof with k = max</span></span>
<span><span class="co">#&gt; Number of lines in the reference table: 1000 ;</span></span>
<span><span class="co">#&gt; Number of calibration point: 100 ;</span></span>
<span><span class="co">#&gt; Range of k values in the original object: k in [2, 20] ;</span></span>
<span><span class="co">#&gt; Number of target observations: 1 ;</span></span>
<span><span class="co">#&gt; Table of median, lower and upper 95% HPD pvalues on 10 bootstrap replicates:</span></span>
<span><span class="co">#&gt;      median  lwr  upr</span></span>
<span><span class="co">#&gt; [1,]  0.045 0.01 0.12</span></span></code></pre></div>
<p>Here, it confirms the result that observation 6 is not rejected by
the prior GoF test. We will further analyze this observation using the
post-inference holdout GoF test.</p>
</div>
<div class="section level2">
<h2 id="post-inference-holdout-gof-test">Post-Inference Holdout GoF Test<a class="anchor" aria-label="anchor" href="#post-inference-holdout-gof-test"></a>
</h2>
<p>The prior GoF tests the null hypothesis that the data comes from the
prior distribution of model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>,
which is a broad assumption. In a frequentist setting, we might want
instead to test whether the true distribution that generated the data is
equal to the optimized likelihood of the model, inferred in the region
of the observation. This is the goal of the post-inference GoF test. To
avoid the double use of the data, we resort to an holdout version of the
test: we assume that we have two independent observed datasets, that
were drawn from the same (unknown) distribution, and we use the first
replicate to learn the posterior, and the second replicate to perform
the test.</p>
<p>In our toy example, as we know the true distribution of observation
6, we can easily generate a replicate dataset, by calling the Gaussian
simulation function on the same parameters:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">y_obs_rep</span> <span class="op">&lt;-</span> <span class="va">dataset_gaussian</span><span class="op">$</span><span class="fu">sim.fun</span><span class="op">(</span><span class="va">dataset_gaussian</span><span class="op">$</span><span class="va">param</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="op">]</span><span class="op">)</span></span></code></pre></div>
<p>Here, the posterior is learned with a simple rejection ABC algorithm,
by selecting the rows of the reference table that are the closest (for
the Euclidean distance) to the first observation, and then by
re-simulating points from model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>
using the parameters associated to the selected rows of the reference
table.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reshpgfit</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hpgfit.html">hpgfit</a></span><span class="op">(</span><span class="va">y_obs</span><span class="op">[</span><span class="fl">6</span>, <span class="op">]</span>,                      <span class="co">## observed dataset</span></span>
<span>                    <span class="va">y_obs_rep</span><span class="op">[</span><span class="fl">6</span>, <span class="op">]</span>,                  <span class="co">## replicate of the observed dataset</span></span>
<span>                    <span class="va">sim_params</span>, <span class="va">sumstats</span>,            <span class="co">## simulated parameters and reference table from m_0</span></span>
<span>                    sim.fun <span class="op">=</span> <span class="va">sim_function</span>,          <span class="co">## function to re-simulate from m_0 given parameters</span></span>
<span>                    method <span class="op">=</span> <span class="st">"rejection"</span>, eps <span class="op">=</span> <span class="fl">0.1</span><span class="op">)</span> <span class="co">## rejection method with epsilon = 10%</span></span>
<span><span class="va">reshpgfit</span></span>
<span><span class="co">#&gt; Holdout Posterior GoF analysis</span></span>
<span><span class="co">#&gt; Using the lof with k = max</span></span>
<span><span class="co">#&gt; Number of lines in the reference table: 1000 ;</span></span>
<span><span class="co">#&gt; Number of lines in the posterior table: 100 (eps=10%);</span></span>
<span><span class="co">#&gt; Number of calibration point: 50 (split=50%);</span></span>
<span><span class="co">#&gt; Range of k values in the original object: k in [2, 20] ;</span></span>
<span><span class="co">#&gt; Number of target observations: 1 ;</span></span>
<span><span class="co">#&gt; Table of estimate, lower and upper 95% confidance interval pvalues:</span></span>
<span><span class="co">#&gt;     estim lwr upr</span></span>
<span><span class="co">#&gt; max     0   0   0</span></span>
<span><span class="co">#&gt; Confidence intervals are based on asymptotic standard error estimation.</span></span></code></pre></div>
<p>Here, the post-inference test does reject the null assumption that
the true distribution of the data comes from model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>m</mi><mn>0</mn></msub><annotation encoding="application/x-tex">m_0</annotation></semantics></math>.
Note that, for the sake of speed in this demonstration, we used a
reference table with only
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math>
particles, localized at
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">10\%</annotation></semantics></math>.
In true applications, for the post-inference test, we should aim at many
more simulated particles
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>10</mn><mspace width="0.167em"></mspace><mn>000</mn></mrow><annotation encoding="application/x-tex">10\,000</annotation></semantics></math>
or higher, depending on the complexity of the model), and well better
localized
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math>
or lower).</p>
<p>As in the prior GoF test, we can use a bootstrap procedure instead of
the asymptotic method to get better estimates of the uncertainty of the
estimated p-value. This works by re-sampling the calibration points
among the set of selected rows of the reference table.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">reshpgfitboot</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hpgfit.html">hpgfit</a></span><span class="op">(</span><span class="va">y_obs</span><span class="op">[</span><span class="fl">6</span>, <span class="op">]</span>,                      <span class="co">## observed dataset</span></span>
<span>                        <span class="va">y_obs_rep</span><span class="op">[</span><span class="fl">6</span>, <span class="op">]</span>,                  <span class="co">## replicate of the observed dataset</span></span>
<span>                        <span class="va">sim_params</span>, <span class="va">sumstats</span>,            <span class="co">## simulated parameters and reference table from m_0</span></span>
<span>                        sim.fun <span class="op">=</span> <span class="va">sim_function</span>,          <span class="co">## function to re-simulate from m_0 given parameters</span></span>
<span>                        method <span class="op">=</span> <span class="st">"rejection"</span>, eps <span class="op">=</span> <span class="fl">0.1</span>, <span class="co">## rejection method with epsilon = 10%</span></span>
<span>                        nboot <span class="op">=</span> <span class="fl">50</span>,                      <span class="co">## For speed, only 50 replicates, but this should be increased.</span></span>
<span>                        ncores <span class="op">=</span> <span class="fl">1</span><span class="op">)</span>                      <span class="co">## The number of cores for parallel computation can be increased.</span></span>
<span><span class="va">reshpgfitboot</span></span>
<span><span class="co">#&gt; Holdout Posterior GoF analysis</span></span>
<span><span class="co">#&gt; Using the lof with k = max</span></span>
<span><span class="co">#&gt; Number of lines in the reference table: 1000 ;</span></span>
<span><span class="co">#&gt; Number of lines in the posterior table: 100 (eps=10%);</span></span>
<span><span class="co">#&gt; Number of calibration point: 50 (split=50%);</span></span>
<span><span class="co">#&gt; Range of k values in the original object: k in [2, 20] ;</span></span>
<span><span class="co">#&gt; Number of target observations: 1 ;</span></span>
<span><span class="co">#&gt; Table of median, lower and upper 95% HPD pvalues on 50 bootstrap replicates:</span></span>
<span><span class="co">#&gt;      median lwr  upr</span></span>
<span><span class="co">#&gt; [1,]   0.02   0 0.04</span></span></code></pre></div>
<p>The conclusion is similar, although the uncertainty is larger, due to
the small number of particles.</p>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Paul Bastide, Arnaud Estoup, Guillaume Le Mailloux, Jean-Michel Marin.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
