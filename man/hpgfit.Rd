% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/hpgfit.R
\name{hpgfit}
\alias{hpgfit}
\alias{print.hpgfit}
\alias{summary.hpgfit}
\alias{print.hpgfitsummary}
\title{Holdout Frequentist Goodness of Fit Test}
\usage{
hpgfit(
  target,
  target.replica,
  param,
  sumstat,
  sim.fun,
  method = c("rejection", "loclinear", "ridge"),
  kernel = c("epanechnikov", "rectangular", "gaussian", "triangular", "biweight",
    "cosine"),
  lambda = c(1e-04, 0.001, 0.01),
  param_transform = "none",
  param_lower_bound = -Inf,
  param_upper_bound = Inf,
  trans.fun = NULL,
  back.trans.fun = NULL,
  score = c("lof", "kNN"),
  k = c(2, 3, 5, 8, 14, 20),
  k_range = range(k),
  eps = 0.01,
  split = 0.5,
  norm = sd,
  ncores = 1,
  nboot = 1,
  ...
)

\method{print}{hpgfit}(x, ...)

\method{summary}{hpgfit}(object, score = "lof", k = "max", level = 0.95, ...)

\method{print}{hpgfitsummary}(x, ...)
}
\arguments{
\item{target}{a matrix of observations, with dimension Nobs x Nstats.}

\item{target.replica}{a matrix of observations, with dimension Nobs x Nstats.
Each line i of the replicate target should come from the same model
as the one used for the same line i of the reference table.}

\item{param}{the matrix of parameters matching the sumstat.}

\item{sumstat}{a matrix of summary statistics from the training set,
with dimension Ntrain x Nstats.}

\item{sim.fun}{a function that, for a matrix of parameters (size nsim x nparams), returns a simulated matrix of sumstats (size nsim x nsumstats).}

\item{method}{one of "rejection" (default), "loclinear" or "ridge".}

\item{kernel}{for "loclinear" and "ridge", the kernel to be used.}

\item{lambda}{for "ridge", the lambda penalties to be used in the ridge regression. The median of all predictions with various lambdas will be used. Default to \code{lambda = c(0.0001, 0.001, 0.01)}.}

\item{param_transform}{a named vector of function, that to each parameter attributes a transformation function.
Each function must be one of "none" (no transformation), "log" or "logit".
If "logit", upper and lower values are taken in \code{param_lower_bound} \code{param_upper_bound} arguments.
If the vector is of length 1, it will be recycled to all parameters.
Default to "none": no transformation on any parameter.}

\item{param_lower_bound}{a named vector of lower bounds for each parameter in \code{param}.
If the vector is of length 1, it will be recycled to all parameters.
Default to \code{-Inf}: no lower bound.}

\item{param_upper_bound}{a named vector of lower bounds for each parameter in \code{param}.
If the vector is of length 1, it will be recycled to all parameters.
Default to \code{Inf}: no upper bound.}

\item{trans.fun}{A custom transformation function, that takes as input the whole vector of parameters.
If specified, it will override the default logit transform. Both \code{trans.fun} and \code{back.trans.fun} must be specified.
Default to NULL.}

\item{back.trans.fun}{A custom back transformation function, that takes as input the whole vector of parameters.
If specified, it will override the default logit transform. Both \code{trans.fun} and \code{back.trans.fun} must be specified.
Default to NULL.}

\item{score}{the score to use for calling outliers. Can be "lof", "kNN", or both.}

\item{k}{The kth-distance to be used to calculate LOFs of kNNs.
k can be a vector which contains multiple k values based on which the score needs to be calculated.}

\item{k_range}{A vector of min and max values for k when using the "LOF max" score (see Details).}

\item{eps}{the proportion of data points used to approximate the posterior distribution
of the reference table around each observation point. Default to 0.01.}

\item{split}{proportion of the posterior to be used for calibration. Default to 0.5.}

\item{norm}{the normalization function. Default to \code{\link{sd}}.}

\item{ncores}{number of cores for parallel computations.}

\item{nboot}{number of bootstrap replicates (see Details). Default to 1: no bootstrap replicate.}

\item{...}{further arguments to be passed to \code{sim.fun}.}

\item{x}{an object used to select a method.}

\item{object}{an object for which a summary is desired.}

\item{level}{the level for the confidence interval}
}
\value{
An object of class \code{hpgfit}. For each score, it contains a list with:
\itemize{
\item \code{pval} a matrix of p-values for the test data (rows), for each possible value of k (if \code{nboot=1}),
or an array of such matrices (if \code{nboot>1}).
\item \code{n.ref} the number of particles in the reference dataset (number of rows of \code{sumstat}).
\item \code{n.calib} the number of particles used for calibration.
\item \code{eps} the localization fraction.
\item \code{split} the fraction of data used for calibration.
\item \code{n.boot} the number of bootstrap replicates.
\item \code{k_range} the min and max values for k when using the "LOF max" score.
}
}
\description{
This function performs a post-inference holdout Goodness of Fit (GoF) test,
that checks whether we can reject the hypothesis that the \code{target}
is from the same model as the particles from the \code{sumstat} matrix.

It works by first approximating the posterior distribution using a
localization parameter \code{eps} and one of the rejection, local linear or ridge regression
methods (see documentation from \code{abc::abc}).
It then draws a fraction \code{split} of the reference table \code{sumstat}
for calibration, and then compares the score of the
\code{target} with the distribution of scores of the calibration points.
}
\details{
The LOF and kNN scores are computations rely on the
function \code{\link[dbscan]{kNN}} from the \code{dbscan} package.

\code{target} can be a matrix, in which case the GoF test is performed independently
for each observation vector (rows of the matrix).

When using the "LOF max" statistics, the maximum LOF score over \code{k_range}
is taken for all calibration and observation points.

When \code{nboot>1}, the test is performed several times, by drawing \code{nboot}
different sets of calibration points from the (fixed) posterior.
Highest Density Intervals are then computed using the
\code{\link[HDInterval]{hdi}} function from package \code{HDInterval}.
}
\examples{
data(gaussian_laplace)

## 1000 simulations from the Laplace dataset
sumstat <- gaussian_laplace$dataset_laplace$sim
## Matching parameters and simulation function
param <- gaussian_laplace$dataset_laplace$param
sim_laplace <- gaussian_laplace$dataset_laplace$sim.fun

## 10 observations from the Gaussian dataset
target <- gaussian_laplace$dataset_gaussian$sim[1:10, ]
## 10 replicate obsertvations from the Gaussian dataset
param_target <- gaussian_laplace$dataset_gaussian$param[1:10, ]
target.rep <- gaussian_laplace$dataset_gaussian$sim.fun(param_target)

## Apply hpgfit
res <- hpgfit(target, target.rep, param, sumstat,
              sim.fun = sim_laplace,
              method = "rejection", eps = 0.1) # rejection method
res

## Apply hpgfit with bootstrap
res <- hpgfit(target, target.rep, param, sumstat,
              sim.fun = sim_laplace,
              method = "rejection", eps = 0.1,
              nboot = 10) # nboot should be larger
res

}
\references{
Le Mailloux G., Bastide, P., Marin, J-M., Estoup, A. (2024+),
Goodness of Fit for Bayesian Generative Models with Applications in Population Genetics.

Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, and Jörg Sander. 2000.
LOF: identifying density-based local outliers. SIGMOD Rec. 29, 2 (June 2000), 93–104.
https://doi.org/10.1145/335191.335388
}
\seealso{
\code{\link[=summary.hpgfit]{summary.hpgfit()}} for summarizing the results;
\code{\link[=gof.fit.holdout]{gof.fit.holdout()}} for prior GoF computation with fixed calibration points;
\code{\link[=gfit]{gfit()}} for pre-inference prior GoF test.
}
